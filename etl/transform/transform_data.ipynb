{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, collections, csv\n",
    "directory_for_json_data = '../data/raw/json_data'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Handling Missing Values\n",
    "\n",
    "Stock Prices\n",
    "\n",
    "We've already pointed out data that has invalid profiles or empty locations in our geocoding jupyter notebook within /pipelines. We'll be resolving the following invalid files in /company_profile and /json_data: ['AIMAU.json', 'BRK.B.json', 'AGM.A.json', 'ALCY.json', 'CRD.A.json', 'CRD.B.json', 'AACT.json', 'GODN.json', 'WHF.json', 'CATC.json', 'MSAC.json', 'BMO.json', 'OAKU.json']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid files: 1141\n",
      "Number of files with missing data: 37\n",
      "Number of files with duplicate data: 0\n",
      "Number of files with empty fields: 0\n",
      "defaultdict(<class 'list'>, {'AIMAU.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'GBRG.json': ['zip'], 'MEGL.json': ['zip'], 'AVAL.json': ['zip'], 'BTWN.json': ['zip'], 'MLAC.json': ['zip'], 'LU.json': ['zip'], 'BMA.json': ['zip'], 'BRK.B.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'AGM.A.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'BSAQ.json': ['zip'], 'ALCY.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'BSAC.json': ['zip'], 'BLX.json': ['zip'], 'CRD.A.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'APCA.json': ['zip'], 'CIB.json': ['zip'], 'BCH.json': ['zip'], 'CRD.B.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'AGBA.json': ['zip'], 'AACT.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'GODN.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'OPA.json': ['zip'], 'DIST.json': ['zip'], 'TOP.json': ['zip'], 'ESNT.json': ['zip'], 'WHF.json': ['latitude', 'longitude', 'address1', 'zip'], 'LHC.json': ['zip'], 'SRL.json': ['zip'], 'CATC.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'HHLA.json': ['zip'], 'AMTD.json': ['zip'], 'FUTU.json': ['zip'], 'ACBA.json': ['zip'], 'MSAC.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'BMO.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge'], 'OAKU.json': ['sector', 'latitude', 'city', 'longBusinessSummary', 'country', 'longitude', 'address1', 'zip', 'industryDisp', 'industry', 'maxAge']})\n"
     ]
    }
   ],
   "source": [
    "# Traverse data in /company_profile\n",
    "dir = '../../data/raw/company_profile'\n",
    "assert len(os.listdir(dir)) == 1178\n",
    "\n",
    "# List of valid files && files with missing data\n",
    "file_to_missing_data = collections.defaultdict(list)  # key: file name, value: list of missing data\n",
    "file_to_duplicate_data = collections.defaultdict(list)  # key: file name, value: list of duplicate data\n",
    "file_to_empty_fields = collections.defaultdict(list)  # key: file name, value: list of empty fields\n",
    "valid_files = [] \n",
    "\n",
    "# Check for missing data, duplicate data, empty fields\n",
    "for filename in os.listdir(dir):\n",
    "    # Required data\n",
    "    req_data = { \"address1\", \"city\", \"zip\", \"country\", \"industry\", \n",
    "        \"industryDisp\", \"sector\", \"longBusinessSummary\", \"maxAge\", \"latitude\", \"longitude\" }\n",
    "\n",
    "    # Open JSON file\n",
    "    f = open(dir + '/' + filename, 'r')\n",
    "    f = json.load(f)\n",
    "\n",
    "    # Check for missing data\n",
    "    for key in f:\n",
    "        if key in req_data:\n",
    "            req_data.remove(key)\n",
    "\n",
    "    if len(req_data) > 0:\n",
    "        file_to_missing_data[filename] = list(req_data)\n",
    "        continue\n",
    "    \n",
    "    # Check for duplicate data\n",
    "    seen_data = set()\n",
    "    for key in f:\n",
    "        if key in seen_data:\n",
    "            file_to_duplicate_data[filename].append(key)\n",
    "        else:\n",
    "            seen_data.add(key)\n",
    "\n",
    "    # Check if the file has empty fields\n",
    "    for key in f:\n",
    "        if f[key] == \"\":\n",
    "            file_to_empty_fields[filename].append(key)\n",
    "            print(\"Empty field in file:\", filename, \"for key:\", key)\n",
    "    \n",
    "    # Valid files\n",
    "    if len(req_data) == 0 and len(file_to_missing_data.get(filename, [])) == 0 and len(file_to_duplicate_data.get(filename, [])) == 0 and len(file_to_empty_fields.get(filename, [])) == 0:\n",
    "        valid_files.append(filename)\n",
    "\n",
    "print(\"Number of valid files:\", len(valid_files))\n",
    "print(\"Number of files with missing data:\", len(file_to_missing_data))\n",
    "print(\"Number of files with duplicate data:\", len(file_to_duplicate_data))\n",
    "print(\"Number of files with empty fields:\", len(file_to_empty_fields))\n",
    "\n",
    "print(file_to_missing_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock_Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid files: 1096\n",
      "Number of files with missing data: 82\n",
      "Number of files with duplicate data: 0\n",
      "Number of files with empty fields: 0\n",
      "defaultdict(<class 'list'>, {'ALCY.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'BRK.B.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'FORL.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'RWOD.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'GLST.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'MSAC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'VMCA.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'PTHR.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'MEGL.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'TENK.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'RFAC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'GENQ.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'JGGC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'MARX.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'SPCM.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'EFHT.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'PLAO.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'HNVR.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'PLTN.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'APLD.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'WAVS.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'DMYY.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'TBMC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'TOP.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'CLRC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'HUDA.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'RACY.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'SKGR.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'YOTA.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'ATMC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'SVII.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'INTR.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'FGMC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'BYNO.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'MSSA.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'SHUA.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'CLIN.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'ATMV.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'EVGR.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'GODN.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'CRD.B.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'MCAC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'FLFV.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'DECA.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'ACAC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'PTWO.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'CRD.A.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'MOBV.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'CETU.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'HMAC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'HSPO.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'BLAC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'SBXC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'HNRA.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'NUBI.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'CRBG.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'RENE.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'LBBB.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'GBBK.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'FG.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'CHEA.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'SKWD.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'PWUP.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'ASCB.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'DIST.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'AFAR.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'GDST.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'PNAC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'ECBK.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'ISRL.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'IVCA.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'GSRM.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'OAKU.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'AIMAU.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'FTII.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'BAM.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'AGM.A.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'AQU.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'EMCG.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'TMTC.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'QOMO.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol'], 'LNKB.csv': ['volume', 'date', 'adjclose', 'low', 'open', 'close', 'high', 'symbol']})\n"
     ]
    }
   ],
   "source": [
    "# Traverse data in /company_profile\n",
    "dir = '../../data/raw/stock_prices'\n",
    "assert len(os.listdir(dir)) == 1178\n",
    "\n",
    "# List of valid files && files with missing data\n",
    "file_to_missing_data = collections.defaultdict(list)  # key: file name, value: list of missing data\n",
    "file_to_duplicate_data = collections.defaultdict(list)  # key: file name, value: list of duplicate data\n",
    "file_to_empty_fields = collections.defaultdict(list)  # key: file name, value: list of empty fields\n",
    "valid_files = []\n",
    "\n",
    "# Check for missing data, duplicate data, empty fields\n",
    "for filename in os.listdir(dir):\n",
    "    # Required data\n",
    "    req_data = {\"symbol\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"adjclose\"}\n",
    "\n",
    "    # Open CSV file\n",
    "    with open(os.path.join(dir, filename), 'r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "\n",
    "        try:\n",
    "            row = next(csv_reader)\n",
    "        except StopIteration:\n",
    "            file_to_missing_data[filename] = list(req_data)\n",
    "            continue\n",
    "\n",
    "        # Check for missing data\n",
    "        for key in row:\n",
    "            if key in req_data:\n",
    "                req_data.remove(key)\n",
    "\n",
    "        if len(req_data) > 0:\n",
    "            file_to_missing_data[filename] = list(req_data)\n",
    "            continue\n",
    "\n",
    "        # Check for duplicate data\n",
    "        seen_data = set()\n",
    "        for key in row:\n",
    "            if key in seen_data:\n",
    "                file_to_duplicate_data[filename].append(key)\n",
    "            else:\n",
    "                seen_data.add(key)\n",
    "\n",
    "        # Check if the file has empty fields\n",
    "        for key in row:\n",
    "            if row[key] == \"\":\n",
    "                file_to_empty_fields[filename].append(key)\n",
    "                print(\"Empty field in file:\", filename, \"for key:\", key)\n",
    "        # Valid files\n",
    "        if len(req_data) == 0 and len(file_to_missing_data.get(filename, [])) == 0 and len(\n",
    "                file_to_duplicate_data.get(filename, [])) == 0 and len(file_to_empty_fields.get(filename, [])) == 0:\n",
    "            valid_files.append(filename)\n",
    "\n",
    "print(\"Number of valid files:\", len(valid_files))\n",
    "print(\"Number of files with missing data:\", len(file_to_missing_data))\n",
    "print(\"Number of files with duplicate data:\", len(file_to_duplicate_data))\n",
    "print(\"Number of files with empty fields:\", len(file_to_empty_fields))\n",
    "\n",
    "print(file_to_missing_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
